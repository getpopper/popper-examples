- hosts: all
  roles:
  - { role: geerlingguy.docker }
- hosts: master
  tasks:
  - name: run spark master
    shell: >
      hostname "{{ hostvars[inventory_hostname]["ansible_default_ipv4"]["address"] }}" &&
      docker run --name spark-master
      --net=host
      -e SPARK_LOCAL_IP="{{ hostvars[inventory_hostname]["ansible_default_ipv4"]["address"] }}"
      -e SPARK_MASTER_HOST=0.0.0.0
      -e ENABLE_INIT_DAEMON=false
      -d
      popperized/spark-master

- hosts: workers
  tasks:
  - name: run spark worker
    shell: >
      docker run --name spark-worker
      --net=host
      -e ENABLE_INIT_DAEMON=false
      -e SPARK_LOCAL_IP='{{hostvars[inventory_hostname]['ansible_default_ipv4']['address']}}'
      -e SPARK_MASTER='spark://{{ hostvars[groups["master"][0]]["ansible_default_ipv4"]["address"] }}:7077'
      -v /tmp/spark_confs:/tmp/spark_confs
      -d
      popperized/spark-worker
- hosts: master
  tasks:
  - name: copy config files
    copy:
      src: ../spark_confs
      dest: /tmp
  - name: run benchmark config A
    shell: >
      docker run --name spark-configA
      -e SPARK_MASTER_HOST='spark://{{ hostvars[groups["master"][0]]["ansible_default_ipv4"]["address"] }}:7077'
      -v /tmp/spark_confs:/tmp/spark_confs
      --entrypoint=/spark-bench/bin/spark-bench.sh
      popperized/spark-master /tmp/spark_confs/classicA.conf
  - name: run benchmark config B
    shell: >
      docker run --name spark-configB
      -e SPARK_MASTER_HOST='spark://{{ hostvars[groups["master"][0]]["ansible_default_ipv4"]["address"] }}:7077'
      -v /tmp/spark_confs:/tmp/spark_confs
      --entrypoint=/spark-bench/bin/spark-bench.sh
      popperized/spark-master /tmp/spark_confs/classicB.conf
- hosts: workers
  tasks:
  - name: Wait until the spark job-A is finished and result is generated
    wait_for:
      path: /tmp/spark_confs/outA.csv/_SUCCESS
  - name: copy from random file A
    shell: >
      cat /tmp/spark_confs/outA.csv/* > /tmp/outA.csv
  - name: Retreive result A
    fetch:
      src: /tmp/outA.csv
      dest: ../results/
      flat: yes
  - name: Wait until the spark job-B is finished and result is generated
    wait_for:
      path: /tmp/spark_confs/outB.csv/_SUCCESS
  - name: copy from random file B
    shell: >
      cat /tmp/spark_confs/outB.csv/* > /tmp/outB.csv
  - name: Retreive result B
    fetch:
      src: /tmp/outB.csv
      dest: ../results/
      flat: yes